{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "210cea9f-93e1-4818-b22b-9d6947c52c43",
   "metadata": {},
   "source": [
    "# üìö Laboratorio 1: Datos No Estructurados - Procesamiento de Video con Python\n",
    "\n",
    "**Asignatura:** Big Data  \n",
    "**Autores:** Juan Gerardo Mendez, David Gomez, Santiago Cardona\n",
    "\n",
    "**Fecha:** Abril de 2025  \n",
    "\n",
    "---\n",
    "\n",
    "## üß© Tecnolog√≠as y Librer√≠as Utilizadas\n",
    "\n",
    "- Python 3.10\n",
    "- OpenCV\n",
    "- DeepFace\n",
    "- Matplotlib\n",
    "- Pandas\n",
    "- NumPy\n",
    "- scikit-learn\n",
    "- ffmpeg-python\n",
    "\n",
    "---\n",
    "\n",
    "## Pasos Desarrollados\n",
    "\n",
    "### 1. Recopilaci√≥n de Datos\n",
    "\n",
    "Se utilizaron 3 trailers:\n",
    "\n",
    "- `HacksawRidge.mp4`\n",
    "- `ToyStory.mp4`\n",
    "- `Superbad.mp4`\n",
    "\n",
    "### 2. Procesamiento de Video\n",
    "\n",
    "- An√°lisis de color promedio por video.\n",
    "- C√°lculo del movimiento promedio entre frames.\n",
    "- Densidad de rostros (n√∫mero de caras detectadas por segundo).\n",
    "- Extracci√≥n de energ√≠a de audio usando FFmpeg.\n",
    "- An√°lisis de emociones frame por frame.\n",
    "\n",
    "### 3. Clasificaci√≥n Autom√°tica\n",
    "\n",
    "- Entrenamiento de un modelo **Random Forest** personalizado.\n",
    "- Se usaron como features:\n",
    "  - Color promedio (`r`, `g`, `b`)\n",
    "  - Movimiento promedio (`avg_movement`)\n",
    "  - Emoci√≥n dominante (`dominant_emotion`)\n",
    "  - Energ√≠a de audio (`audio_energy`)\n",
    "  - Densidad de caras (`face_density`)\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Implementaciones Destacadas\n",
    "\n",
    "| M√≥dulo                         | Descripci√≥n |\n",
    "|:-------------------------------|:------------|\n",
    "| `analyze_dominant_color()`      | Extrae color promedio del video. |\n",
    "| `analyze_video_movement()`      | Calcula el movimiento entre frames. |\n",
    "| `analyze_audio_energy()`        | Analiza volumen promedio del audio. |\n",
    "| `calculate_face_density()`      | Mide cu√°ntas caras aparecen por segundo. |\n",
    "| `analyze_video_and_write_output()` | Analiza emociones en cada frame y genera un nuevo video anotado. |\n",
    "| `train_genre_classifier()`      | Entrena el modelo de predicci√≥n de g√©neros. |\n",
    "| `predict_genre_with_model()`    | Predice el g√©nero del trailer procesado. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6127cf5b-6002-4ea1-9b5c-daf87cad43a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install opencv-python deepface matplotlib pandas ffmpeg-python numpy scikit-learn moviepy tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05096922-9bb5-488e-9af4-8810c2449b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from deepface import DeepFace\n",
    "import ffmpeg\n",
    "\n",
    "from moviepy import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe05f8bd-81dc-4848-a99f-4ac6654868a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_video_and_write_output(input_path, output_path=\"output_emotion_video.mp4\", frame_sample_rate=5):\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    emotion_data = []\n",
    "\n",
    "    print(f\"Processing {total_frames} frames...\")\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if i % frame_sample_rate == 0:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5)\n",
    "\n",
    "            for (x, y, w, h) in faces:\n",
    "                if w < 30 or h < 30:\n",
    "                    continue  \n",
    "\n",
    "                face_crop = frame[y:y+h, x:x+w]\n",
    "                face_crop = cv2.resize(face_crop, (224, 224))\n",
    "\n",
    "                try:\n",
    "                    result = DeepFace.analyze(\n",
    "                        face_crop, \n",
    "                        actions=[\"emotion\"], \n",
    "                        enforce_detection=False,\n",
    "                        detector_backend=\"retinaface\"  \n",
    "                    )\n",
    "                    dominant_emotion = result[0][\"dominant_emotion\"]\n",
    "                    emotion_data.append(result[0][\"emotion\"])\n",
    "\n",
    "                    # Dibujar la caja y emoci√≥n sobre el frame\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, dominant_emotion, (x, y - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Frame {i}: error analyzing face ‚Äì {e}\")\n",
    "                    continue\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        if i % 60 == 0:\n",
    "            print(f\"Processed frame {i}/{total_frames}\")\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"‚úÖ Video saved at: {output_path}\")\n",
    "\n",
    "    if emotion_data:\n",
    "        df_emotions = pd.DataFrame(emotion_data)\n",
    "        avg_emotions = df_emotions.mean()\n",
    "        print(\"\\n=== Emotion Averages ===\")\n",
    "        print(avg_emotions)\n",
    "        return avg_emotions\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No emotion data was collected.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695fa765-8283-4e9e-be22-10b0541c738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dominant_color(video_path, sample_rate_seconds=5):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = total_frames / fps\n",
    "\n",
    "    print(f\"Video duration: {duration:.2f} seconds\")\n",
    "\n",
    "    timestamps = list(range(0, int(duration), sample_rate_seconds))\n",
    "    avg_colors = []\n",
    "\n",
    "    for second in timestamps:\n",
    "        cap.set(cv2.CAP_PROP_POS_MSEC, second * 1000)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        # Resize to make processing faster\n",
    "        frame_small = cv2.resize(frame, (100, 100))\n",
    "        \n",
    "        # Average color in BGR\n",
    "        avg_color_per_row = np.average(frame_small, axis=0)\n",
    "        avg_color = np.average(avg_color_per_row, axis=0)\n",
    "        avg_colors.append(avg_color)  # BGR format\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    avg_colors = np.array(avg_colors)\n",
    "\n",
    "    # Average across all sampled frames\n",
    "    mean_color_bgr = np.mean(avg_colors, axis=0)\n",
    "\n",
    "    # Convert BGR to RGB correctly\n",
    "    mean_color_rgb = mean_color_bgr[::-1]\n",
    "\n",
    "    print(f\"Average color (RGB): {mean_color_rgb}\")\n",
    "\n",
    "    color_square = np.ones((100, 100, 3), dtype=np.uint8)\n",
    "    color_square[:, :] = np.clip(mean_color_rgb, 0, 255).astype(np.uint8)\n",
    "\n",
    "    plt.imshow(color_square)\n",
    "    plt.title(f\"Average Color of Video\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    return mean_color_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a10594-d52b-4542-8e27-b6c9b1378b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_video_movement(video_path, sample_rate_frames=5):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Total frames: {total_frames}\")\n",
    "\n",
    "    prev_gray = None\n",
    "    movement_scores = []\n",
    "\n",
    "    for i in range(0, total_frames, sample_rate_frames):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if prev_gray is not None:\n",
    "            diff = cv2.absdiff(prev_gray, gray)\n",
    "            score = np.sum(diff) / diff.size  \n",
    "            movement_scores.append(score)\n",
    "\n",
    "        prev_gray = gray\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(movement_scores) == 0:\n",
    "        print(\"‚ö†Ô∏è No movement data collected.\")\n",
    "        return 0\n",
    "\n",
    "    avg_movement = np.mean(movement_scores)\n",
    "    print(f\"Average movement score: {avg_movement}\")\n",
    "\n",
    "    return avg_movement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0421f481-4f41-4693-abf2-e5e5af4b4662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_audio_energy(video_path):\n",
    "    try:\n",
    "        clip = VideoFileClip(video_path)\n",
    "\n",
    "        if clip.audio is None:\n",
    "            print(\"‚ö†Ô∏è No audio track found.\")\n",
    "            return 0.0\n",
    "\n",
    "        # Extract audio as an array\n",
    "        audio_array = clip.audio.to_soundarray(fps=44100) \n",
    "        \n",
    "        if audio_array.ndim == 2:\n",
    "            audio_array = audio_array.mean(axis=1)\n",
    "\n",
    "        # Calculate RMS (Root Mean Square) energy\n",
    "        audio_energy = np.sqrt(np.mean(audio_array**2))\n",
    "\n",
    "        print(f\"Audio Energy: {audio_energy:.4f}\")\n",
    "        return audio_energy\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error analyzing audio: {e}\")\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44db4fff-4114-44d5-8b1f-7b51cbc25f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def calculate_face_density(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = total_frames / fps\n",
    "\n",
    "    face_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "        face_count += len(faces)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    face_density = face_count / duration  \n",
    "    return face_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e957ad-af80-4cff-a847-417527b0542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_genre_classifier():\n",
    "    data = {\n",
    "        \"r\": [\n",
    "            70, 180, 120, 50, 250, 90, 40, 210, 200, 80,\n",
    "            100, 60, 160, 150, 130, 180, 100, 70, 90, 60,\n",
    "            220, 230, 240, 200, 210, 90, 80, 75, 85, 65,\n",
    "            60, 70, 55, 45, 80, 90, 100, 110, 70, 80,\n",
    "            180, 170, 190, 200, 210, 220, 120, 130, 140, 150\n",
    "        ],\n",
    "        \"g\": [\n",
    "            70, 60, 140, 60, 200, 110, 60, 170, 180, 60,\n",
    "            80, 50, 140, 130, 110, 190, 120, 80, 100, 70,\n",
    "            230, 220, 210, 200, 190, 110, 100, 95, 105, 85,\n",
    "            60, 75, 55, 45, 90, 100, 110, 120, 75, 85,\n",
    "            160, 150, 170, 180, 190, 200, 140, 130, 120, 110\n",
    "        ],\n",
    "        \"b\": [\n",
    "            90, 50, 100, 50, 180, 130, 70, 160, 170, 70,\n",
    "            90, 60, 120, 110, 90, 170, 100, 90, 110, 80,\n",
    "            240, 250, 230, 220, 210, 130, 120, 115, 125, 105,\n",
    "            70, 85, 65, 55, 100, 110, 120, 130, 85, 95,\n",
    "            140, 130, 150, 160, 170, 180, 110, 100, 90, 80\n",
    "        ],\n",
    "        \"avg_movement\": [\n",
    "            15, 20, 10, 5, 22, 18, 7, 21, 19, 6,\n",
    "            12, 8, 17, 16, 9, 23, 11, 7, 13, 9,\n",
    "            20, 18, 21, 19, 22, 10, 11, 9, 12, 8,\n",
    "            7, 6, 5, 4, 15, 16, 17, 18, 6, 8,\n",
    "            21, 20, 22, 23, 24, 25, 13, 14, 15, 16\n",
    "        ],\n",
    "        \"dominant_emotion\": [\n",
    "            3, 2, 5, 4, 1, 5, 4, 2, 1, 3,\n",
    "            3, 4, 2, 4, 5, 1, 3, 4, 3, 5,\n",
    "            1, 2, 3, 4, 5, 2, 1, 5, 4, 3,\n",
    "            2, 1, 5, 4, 3, 2, 1, 5, 4, 3,\n",
    "            1, 2, 3, 4, 5, 1, 2, 3, 4, 5\n",
    "        ],\n",
    "        \"audio_energy\": [\n",
    "            0.22, 0.48, 0.30, 0.10, 0.50, 0.28, 0.09, 0.45, 0.42, 0.12,\n",
    "            0.24, 0.18, 0.39, 0.36, 0.20, 0.49, 0.25, 0.14, 0.26, 0.17,\n",
    "            0.48, 0.47, 0.50, 0.45, 0.49, 0.32, 0.31, 0.33, 0.30, 0.29,\n",
    "            0.28, 0.27, 0.26, 0.25, 0.35, 0.34, 0.33, 0.32, 0.24, 0.23,\n",
    "            0.50, 0.49, 0.48, 0.47, 0.46, 0.45, 0.36, 0.35, 0.34, 0.33\n",
    "        ],\n",
    "        \"face_density\": [\n",
    "            1.2, 1.5, 1.8, 0.4, 1.7, 1.3, 0.5, 1.6, 1.4, 0.6,\n",
    "            1.0, 0.8, 1.5, 1.7, 0.9, 1.9, 1.1, 0.7, 1.2, 0.8,\n",
    "            2.1, 2.3, 2.0, 1.8, 2.2, 1.1, 1.3, 1.4, 1.2, 1.5,\n",
    "            0.6, 0.7, 0.8, 0.9, 1.7, 1.6, 1.5, 1.4, 0.7, 0.9,\n",
    "            2.4, 2.3, 2.2, 2.1, 2.0, 1.9, 1.5, 1.4, 1.3, 1.2\n",
    "        ],\n",
    "        \"genre\": [\n",
    "            \"Action\", \"Action\", \"Animation\", \"Drama\", \"Action\",\n",
    "            \"Comedy\", \"Drama\", \"Sci-Fi\", \"Sci-Fi\", \"Comedy\",\n",
    "            \"Fantasy\", \"Drama\", \"Sci-Fi\", \"Fantasy\", \"Comedy\",\n",
    "            \"Action\", \"Crime\", \"Drama\", \"Animation\", \"Horror\",\n",
    "            \"Action\", \"Action\", \"Animation\", \"Sci-Fi\", \"Sci-Fi\",\n",
    "            \"Comedy\", \"Comedy\", \"Fantasy\", \"Fantasy\", \"Drama\",\n",
    "            \"Drama\", \"Crime\", \"Crime\", \"Horror\", \"Horror\",\n",
    "            \"Action\", \"Action\", \"Animation\", \"Animation\", \"Comedy\",\n",
    "            \"Sci-Fi\", \"Sci-Fi\", \"Fantasy\", \"Fantasy\", \"Crime\",\n",
    "            \"Crime\", \"Horror\", \"Horror\", \"Drama\", \"Drama\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    X = df.drop(\"genre\", axis=1)\n",
    "    y = df[\"genre\"]\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=150, max_depth=8, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"\\n=== Model Performance ===\")\n",
    "    print(classification_report(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        labels=le.transform(le.classes_),\n",
    "        target_names=le.classes_\n",
    "    ))\n",
    "\n",
    "    return model, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd9888-ef6c-465e-995b-de8e65c56df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_genre_with_model(mean_color_rgb, avg_movement, avg_emotions, audio_energy, face_density, model, label_encoder):\n",
    "    r, g, b = mean_color_rgb\n",
    "    dominant_emotion_name = avg_emotions.idxmax()\n",
    "\n",
    "    emotion_mapping = {\n",
    "        \"angry\": 1,\n",
    "        \"disgust\": 2,\n",
    "        \"fear\": 3,\n",
    "        \"happy\": 4,\n",
    "        \"sad\": 5,\n",
    "        \"surprise\": 6,\n",
    "        \"neutral\": 7\n",
    "    }\n",
    "    dominant_emotion_mapped = emotion_mapping.get(dominant_emotion_name.lower(), 0)\n",
    "\n",
    "    features = pd.DataFrame([{\n",
    "        \"r\": r,\n",
    "        \"g\": g,\n",
    "        \"b\": b,\n",
    "        \"avg_movement\": avg_movement,\n",
    "        \"dominant_emotion\": dominant_emotion_mapped,\n",
    "        \"audio_energy\": audio_energy,\n",
    "        \"face_density\": face_density\n",
    "    }])\n",
    "\n",
    "    predicted = model.predict(features)\n",
    "    predicted_genre = label_encoder.inverse_transform(predicted)[0]\n",
    "    return predicted_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a6f636-6d71-4498-a08c-b7a85f8b2ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_trailers(video_paths, model, label_encoder):\n",
    "    results = []\n",
    "\n",
    "    for video_path in video_paths:\n",
    "        print(f\"\\nüöÄ Procesando video: {video_path}\")\n",
    "\n",
    "        mean_color = analyze_dominant_color(video_path)\n",
    "        avg_movement = analyze_video_movement(video_path)\n",
    "        audio_energy = analyze_audio_energy(video_path)\n",
    "        face_density = calculate_face_density(video_path)\n",
    "        avg_emotions = analyze_video_and_write_output(video_path)\n",
    "\n",
    "        if avg_emotions is not None:\n",
    "            genre = predict_genre_with_model(\n",
    "                mean_color,\n",
    "                avg_movement,\n",
    "                avg_emotions,\n",
    "                audio_energy,\n",
    "                face_density,\n",
    "                model,\n",
    "                label_encoder\n",
    "            )\n",
    "\n",
    "            results.append({\n",
    "                \"video\": video_path,\n",
    "                \"r\": mean_color[0],\n",
    "                \"g\": mean_color[1],\n",
    "                \"b\": mean_color[2],\n",
    "                \"avg_movement\": avg_movement,\n",
    "                \"dominant_emotion\": avg_emotions.idxmax(),\n",
    "                \"audio_energy\": audio_energy,\n",
    "                \"face_density\": face_density,\n",
    "                \"predicted_genre\": genre\n",
    "            })\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9eef18-a508-4341-a7f5-99048404f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emotions_summary(df_results):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    df_results['dominant_emotion'].value_counts().plot(kind='bar', color='skyblue')\n",
    "    plt.title('Dominant Emotion per Trailer')\n",
    "    plt.xlabel('Emotion')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c0b84-9a3f-4883-bf8e-5b305dcfc282",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, label_encoder = train_genre_classifier()\n",
    "\n",
    "video_paths = [\n",
    "    \"NoRespires.mp4\"\n",
    "]\n",
    "\n",
    "df_final_results = process_multiple_trailers(video_paths, model, label_encoder)\n",
    "\n",
    "print(\"\\nüé¨üìä Resultados Finales:\")\n",
    "print(df_final_results)\n",
    "plot_emotions_summary(df_final_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
